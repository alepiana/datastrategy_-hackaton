{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "def load_datasets(dataset_names, base_path):\n",
    "    datasets = {}\n",
    "    for name in dataset_names:\n",
    "        file_path = os.path.join(base_path, name + '.csv')\n",
    "        try:\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            datasets[name] = None\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assuming /kaggle/input/brazilian-ecommerce/ is the correct directory\n",
    "base_path = '/kaggle/input/brazilian-ecommerce/'\n",
    "\n",
    "# Dataset names\n",
    "dataset_names = [\n",
    "    'olist_customers_dataset', \n",
    "    'olist_order_items_dataset', \n",
    "    'olist_order_payments_dataset', \n",
    "    'olist_order_reviews_dataset', \n",
    "    'olist_orders_dataset', \n",
    "    'olist_sellers_dataset', \n",
    "    'olist_products_dataset', \n",
    "    'product_category_name_translation'\n",
    "]\n",
    "\n",
    "# Assuming you have a function load_datasets that loads the datasets given the names and path\n",
    "datasets = load_datasets(dataset_names, base_path)\n",
    "\n",
    "# Displaying the first few rows of each dataset to understand their structure\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\nFirst few rows of {name}:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"Failed to load {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_datasets(datasets):\n",
    "    # Start by merging orders with customers\n",
    "    merged = pd.merge(datasets['olist_orders_dataset'], datasets['olist_customers_dataset'], on='customer_id', how='left')\n",
    "\n",
    "    # Add other datasets with the correct merge keys\n",
    "    merge_keys = {\n",
    "        'order_items': 'order_id',\n",
    "        'order_payments': 'order_id',\n",
    "        'order_reviews': 'order_id',\n",
    "        'sellers': 'seller_id',\n",
    "        'products': 'product_id',\n",
    "        'product_category_name_translation': 'product_category_name'  # Make sure this dataset is loaded correctly\n",
    "    }\n",
    "\n",
    "    for name, key in merge_keys.items():\n",
    "        if name in datasets:\n",
    "            merged = pd.merge(merged, datasets[name], on=key, how='left')\n",
    "\n",
    "    return merged\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load individual datasets\n",
    "olist_customers_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv')\n",
    "order_items_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv')\n",
    "order_payments_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv')\n",
    "order_reviews_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv')\n",
    "olist_orders_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv')\n",
    "sellers_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv')\n",
    "products_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv')\n",
    "product_category_name_translation_df = pd.read_csv('/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv')\n",
    "\n",
    "# Define the datasets dictionary\n",
    "datasets = {\n",
    "    'olist_customers_dataset': olist_customers_df,\n",
    "    'order_items': order_items_df,\n",
    "    'order_payments': order_payments_df,\n",
    "    'order_reviews': order_reviews_df,\n",
    "    'olist_orders_dataset': olist_orders_df,\n",
    "    'sellers': sellers_df,\n",
    "    'products': products_df,\n",
    "    'product_category_name_translation': product_category_name_translation_df\n",
    "}\n",
    "\n",
    "# Now, you can call the merge_datasets function with the loaded datasets\n",
    "merged_df = merge_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying data cleaning to new csv file\n",
    "merged_df.to_csv('olist_merged_data.csv', index=False)\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values by percentage in each column\n",
    "merged_df.isnull().sum() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with more than 50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \n",
    "                    'order_delivered_customer_date', 'order_estimated_delivery_date', \n",
    "                    'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate new features\n",
    "    df['time_to_delivery'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.days\n",
    "    df['order_processing_time'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.days\n",
    "    df['estimated_vs_actual_shipping'] = (df['order_estimated_delivery_date'] - df['order_delivered_customer_date']).dt.days\n",
    "    df['product_volume_m3'] = (df['product_length_cm'] * df['product_width_cm'] * df['product_height_cm']) / 1000000\n",
    "    df['satisfaction'] = (df['review_score'] >= 4).astype(int)\n",
    "    df['order_value'] = df['price'] + df['freight_value']\n",
    "\n",
    "    # create late delivery flag\n",
    "    df['late_delivery'] = (df['order_delivered_customer_date'] > df['order_estimated_delivery_date']).astype(int)\n",
    "\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # create seasonal features from order_purchase_timestamp\n",
    "    df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['order_day'] = df['order_purchase_timestamp'].dt.dayofweek\n",
    "    df['order_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "merged_df = preprocess_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "merged_df.drop(['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'review_score', 'seller_zip_code_prefix']\n",
    "               , axis=1, inplace=True) \n",
    "# save the cleaned dataset\n",
    "merged_df.to_csv('olist_merged_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "merged_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
